{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[homework]seq2seq_and_attn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jgH3CAcubM4z"
      },
      "source": [
        "# Hausaufgabe\n",
        "translate(Hausaufgabe) -> **Homework**\n",
        "\n",
        "Halo!\n",
        "На семинаре мы создали простую seq2seq модель на основе rnn для перевода, а сейчас постараемся засунуть туда attention. Работать будем с тем же датасетом DE->EN (датасеты получше просто не влезают в память колаба, но если у вас есть CPU+тонна времени или GPU побольше, то можно попробовать построить перевод на WMT14 или IWSLT )\n",
        "\n",
        "В конце домашней работы предполагается написание отчета о проделанной работе."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fPuwHEnVIzn",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchtext.datasets import TranslationDataset, Multi30k #WMT14, IWSLT\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQSnhb84VLU7",
        "colab": {}
      },
      "source": [
        "seed = 43\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L10vdpVaVXBo",
        "outputId": "eac766d0-1c7d-47fc-8649-104bfb004ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! python -m spacy download en\n",
        "! python -m spacy download de\n",
        "\n",
        "\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 678kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=fd4e7c843365023476e0783f2d6be48180d1c25f84d353ee7aab00c5dda9355a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2q2652xj/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ferOqkOUVirW",
        "colab": {}
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# немецкий язык является полем SRC, а английский в поле TRG\n",
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u6pNY6cWW3j5",
        "outputId": "7f6a8203-2226-432a-fff2-faca6b86ba8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 321kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 91.9kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 88.2kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOS3e7QZbLro"
      },
      "source": [
        "Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r0Xpf4IBW4Uf",
        "outputId": "dff1ceb3-60e2-44a9-d195-488acc4235ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "labels = ['train', 'validation', 'test']\n",
        "dataloaders = [train_data, valid_data, test_data]\n",
        "for d, l in zip(dataloaders, labels):\n",
        "    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in train : 29000\n",
            "Number of sentences in validation : 1014\n",
            "Number of sentences in test : 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gg63m8haW4XC",
        "outputId": "e2b8cbf3-09ba-4ba4-8bc1-fa7636319d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "print(\"Number of words in source vocabulary\", len(SRC.vocab))\n",
        "print(\"Number of words in source vocabulary\", len(TRG.vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in source vocabulary 7855\n",
            "Number of words in source vocabulary 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LSd3la5FbJ5_"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "Энкодер будет ровно как в семинаре, с кдинственным изменением -- forward будет возвращать не только hidden, cell, но еще и outputs. Это нужно (надеюсь, вы уже поняли) для использования attention в декодере"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Ar5SN6tW4ck",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional = False):\n",
        "        \"\"\"\n",
        "        :param: input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
        "        :param: emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
        "        :param: hid_dim is the dimensionality of the hidden and cell states.\n",
        "        :param: n_layers is the number of layers in the RNN.\n",
        "        :param: percentage of the dropout to use\n",
        "        \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        " \n",
        "        self.rnn = nn.LSTM(input_size=emb_dim, num_layers = n_layers, hidden_size=hid_dim, bidirectional = bidirectional) #попробовать поменять на lstm или gru\n",
        "                                                                                           #не забывать про bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        :param: src sentences (src_len x batch_size)\n",
        "        \"\"\"\n",
        "        # embedded = <TODO> (src_len x batch_size x embd_dim)\n",
        "        embedded = self.embedding(src)\n",
        "        # dropout over embedding\n",
        "        embedded = self.dropout(embedded)\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # [Attention return is for lstm, but you can also use gru]\n",
        "        return outputs, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-8QOCpKxfD3M"
      },
      "source": [
        "## Decoder\n",
        "Оп ля, а тут уже что-то новенькое\n",
        "\n",
        "Мы будем реализовывать attention, который будет смотреть из tgt в src (НЕ self-attention). \n",
        "\n",
        "Определим два класса -- Attention и DecoderAttn. Мы разбили их на два класса, чтобы можно было играться с типом внимания, не меняя код DecoderAttn. Как вы помните с лекции, в качестве аттеншена можно брать любую странную функцию (конкатенация, маленькая сеточка, ...), и все будет работать! Поэтому вам предлагается попробовать несколько разных.\n",
        "\n",
        "\n",
        "---------------------\n",
        "Есть два подхода к реализации аттеншена:\n",
        "\n",
        "Подход #1:\n",
        "\n",
        "1. Вычисляется embed\n",
        "2. На основе hidden c прошлого шага, embedded и (возможно) enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!). Размерность batch_size * max_len, max_len -- максимальная длина предложения в батче, т.е. shape[0] от выхода энкодера.\n",
        "3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n",
        "4. Берутся attention и embedded и сворачиваются в один вектор размерности такой, чтобы кормить его self.lstm. Например, это можно сделать с помощью обычного линейного слоя\n",
        "5. Вычисляется новое скрытое состояние new_hidden. Это наша self.lstm, примененная к выходу пункта 4.\n",
        "6. Вычисляется prediction, как в семинаре\n",
        "\n",
        "Грубо говоря, вся разница с семинаром в том, что мы вместо того, чтобы embedded пихать в self.lstm, миксуем аттэншен на основе всего, что имеем (enc_out, hidden, embedded) и запихиваем в self.lstm микс аттэншена и embedded.\n",
        "\n",
        "![alt text](https://i.imgur.com/cmkRY0r.png)\n",
        "\n",
        "\n",
        "Подход #2:\n",
        "\n",
        "1. Вычисляется embed\n",
        "2. Вычисляется output, new_hidden (строчка output, (hidden, cell) = self.rnn(embedded, (hidden, cell)))\n",
        "3. На основе output и enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!)\n",
        "3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n",
        "4. Вычисляется prediction на основе attention и output. Можно, например, взять nn.Linear() от конкатенации attention и output.\n",
        "\n",
        "Разница с первым подходом в том, что мы сначала вычисляем выход rnn слоя, а потом смотрим вниманием на src и на основе выхода rnn и attn считаем выход (prediction). \n",
        "\n",
        "![alt text](https://i.imgur.com/5aWjQWv.png)\n",
        "\n",
        "\n",
        "Вам предлагается реализовать хотя бы 1 из вариантов и хотя бы 2 варианта функции attention (в классе Attention)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiw71hH3_O39",
        "colab_type": "text"
      },
      "source": [
        "**Attention метод 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gRgtzaf4bJp6",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,n_layers,hid_dim,device, method=\"one-layer-net\", bidirectional = False): # add parameters needed for your type of attention\n",
        "        super(Attention, self).__init__()\n",
        "        self.method = method # attention method you'll use. e.g. \"cat\", \"one-layer-net\", \"dot\", ...\n",
        "        self.linear = nn.Linear(2*hid_dim*(bidirectional+1), 1)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hid_dim = hid_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.device = device\n",
        "    #сначала разберем для одного слоя и bidirectional = False\n",
        "        \n",
        "    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n",
        "        last_hidden = last_hidden[-(1+self.bidirectional):]\n",
        "        if self.bidirectional:\n",
        "          last_hidden = torch.cat((last_hidden[0],last_hidden[1]), dim = -1)\n",
        "\n",
        "        if self.method == 'one-layer-net':\n",
        "          \"\"\"input = torch.zeros((encoder_outputs.shape[0], encoder_outputs.shape[1], 1)).to(self.device)#переделать, размер батча не всегда одинаковый\n",
        "          for ix in range(len(encoder_outputs)):\n",
        "            input[ix] = self.linear(torch.cat((encoder_outputs[ix], last_hidden), dim = -1))\"\"\"\n",
        "          input = self.linear(torch.cat((encoder_outputs, last_hidden.repeat(encoder_outputs.shape[0],1).reshape(encoder_outputs.shape)), dim = -1))\n",
        "          return F.softmax(input, dim = 0) #надо еще раз посмотреть потому что веса(1,1,1,1)\n",
        "\n",
        "        elif self.method == 'dot':\n",
        "          input = torch.sum(last_hidden.unsqueeze(0)*encoder_outputs, dim = -1).unsqueeze_(-1)\n",
        "          return F.softmax(input, dim = 0)\n",
        "\n",
        "        else:\n",
        "          raise NameError('method does not exist')\n",
        "        \n",
        "class DecoderAttn(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, attention, bidirectional = False, dropout=0.1):\n",
        "        super(DecoderAttn, self).__init__()\n",
        "        \n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        self.attn = attention # instance of Attention class\n",
        "\n",
        "        # define layers\n",
        "        self.embedding = nn.Embedding(output_dim,emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional = bidirectional) #(lstm embd, hid, layers, dropout), попробовать gru\n",
        "        self.out = nn.Linear(self.hid_dim*(1+bidirectional), self.output_dim) # Projection :hid_dim x output_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #self.embed_linear = nn.Linear(self.hid_dim*(1+bidirectional) + self.emb_dim, self.hid_dim)\n",
        "\n",
        "        # more layers you'll need for attention\n",
        "        \n",
        "    def forward(self, input, hidden, cell, encoder_output):\n",
        "        # make decoder with attention\n",
        "        # use code from seminar notebook as base and add attention to it\n",
        "        input_ = input.unsqueeze(0)\n",
        "        \n",
        "        # (1 x batch_size x emb_dim)\n",
        "        embedded = self.embedding(input_)# embd over input and dropout \n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = self.attn(hidden, encoder_output)\n",
        "        new_hidden = torch.sum(encoder_output*attn_weights, dim = 0) # (num_layers * num_directions, batch, hidden_size)\n",
        "        new_hidden = new_hidden.repeat(self.n_layers,1)\n",
        "        #assert hidden.shape == new_hidden.shape\n",
        "        #new_hidden = self.embed_linear(torch.cat((new_hidden, embedded), dim = -1))#для n_layers > 1 наверное придется продублировать 1 ось\n",
        "        #output, (hidden, cell) = self.rnn(embedded, (torch.cat((new_hidden, new_hidden), dim =0) if self.bidirectional else new_hidden, cell)) #сюда надо подавать ((1+bidirectional), batch_size, ...)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (new_hidden.reshape(self.n_layers*(1+self.bidirectional), -1, int(new_hidden.shape[-1]/2)), cell))\n",
        "        prediction = self.out(output.squeeze(0))\n",
        "        return prediction, hidden, cell, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIPC03x_meHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.randn(2, 32, 2*30)\n",
        "b = torch.randn(2, 32, 2*30)\n",
        "b.repeat(2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UlD7-nusfL86"
      },
      "source": [
        "## Seq2Seq module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HOineJlHpof2"
      },
      "source": [
        "Здесь опять ничего не поменяется кроме того, что энкодер теперь возвращает свой output, а декодер его принимает"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stX2lXm5YYJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS_IDX = SRC.vocab.stoi['<sos>']\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        # Hidden dimensions of encoder and decoder must be equal\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self._init_weights() \n",
        "    \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \"\"\"\n",
        "        :param: src (src_len x batch_size)\n",
        "        :param: tgt\n",
        "        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        out,hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            \n",
        "            output, hidden, cell, _ = self.decoder(input, hidden, cell, out) #TODO pass state and input throw decoder \n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            input = (trg[t] if teacher_force else top1)\n",
        "        \n",
        "        return outputs\n",
        "    \n",
        "    def translate(self, src, input):   \n",
        "\n",
        "        max_len = input.shape[0]\n",
        "        batch_size = input.shape[1]\n",
        "     \n",
        "        outputs = torch.zeros(max_len, batch_size, self.decoder.output_dim).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        src = torch.tensor(src).to(self.device)\n",
        "        out, hidden, cell =  self.encoder(src)# TODO pass src throw encoder\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = input[0, :]\n",
        "        weights = []\n",
        "        \n",
        "        model.eval()\n",
        "        for t in range(1, max_len):\n",
        "            with torch.no_grad():\n",
        "              output, hidden, cell, weights_ = self.decoder(input, hidden, cell, out) #TODO pass state and input throw decoder \n",
        "            outputs[t] = output\n",
        "            top1 = output.max(1)[1]\n",
        "            input = (top1)\n",
        "            weights.append(weights_)\n",
        "        \n",
        "        return outputs, weights\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        p = 0.08\n",
        "        for name, param in self.named_parameters():\n",
        "            nn.init.uniform_(param.data, -p, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "msbn2VypfUur",
        "colab": {}
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "output_dim = len(TRG.vocab)\n",
        "src_embd_dim =  tgt_embd_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers =  1\n",
        "dropout_prob = 0.5\n",
        "\n",
        "batch_size = 64\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                  batch_size = batch_size, device = device)\n",
        "train_iterator, valid_iterator, test_iterator = iterators\n",
        "\n",
        "enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob, True)\n",
        "attn = Attention(num_layers, hidden_dim,device, bidirectional=True)#попробовать поиграться с attention\n",
        "dec = DecoderAttn(output_dim, tgt_embd_dim, hidden_dim, num_layers, attn,True, dropout_prob)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h5V9ZnK4fUxq",
        "outputId": "c2c1167d-af16-4ed3-8ef3-d4bacf4c5312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): DecoderAttn(\n",
              "    (attn): Attention(\n",
              "      (linear): Linear(in_features=2048, out_features=1, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, bidirectional=True)\n",
              "    (out): Linear(in_features=1024, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3vaUeDjTfU4k",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output= model(src, trg)\n",
        "        \n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfbTx2FMjaIM",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing !!\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d40KW2NYCMFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import telegram_send\n",
        "#!telegram-send --configure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lQV_yqkLjcyQ",
        "outputId": "d9a330b3-e327-4223-ef9e-e183fe94d291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_epochs = 15\n",
        "CLIP = 1\n",
        "\n",
        "# TODO\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay = 1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.5)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "history_val = []\n",
        "history_train = []\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    \n",
        "    \n",
        "    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
        "    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n",
        "    history_val.append(valid_loss)\n",
        "    history_train.append(train_loss)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.plot(history_train, label = 'train_loss')\n",
        "    plt.plot(history_val, label = 'valid_loss')\n",
        "    \"\"\"plt.savefig('image.jpg')\n",
        "    !telegram-send --image image.jpg \"\"\"\n",
        "    plt.show()\n",
        "\n",
        "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
        "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))\n",
        "#!telegram-send 'done'"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \n",
            " Train Loss 4.8185  Val loss 4.89045:\n",
            "Train Perplexity 123.77928253467711  Val Perplexity 133.01341662317475:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-eedd766ac77e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mhistory_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-c00393fe03bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McAubjZFn5BI",
        "colab_type": "code",
        "outputId": "25ca73bd-0111-40a1-944e-c2bb85be90a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('model_dot_1layer_bidirectional.pt'))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n5Zf6Kb1jhOI",
        "outputId": "2e927dbf-ca12-4a75-cf96-cd65472a7344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 4.841659724712372 Test PPL:126.67943032663294|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "an6HG7_uyjJN",
        "colab": {}
      },
      "source": [
        "EOS_IDX = SRC.vocab.stoi['<eos>']\n",
        "\n",
        "def translate(sentence, max_size = 10):\n",
        "    \"\"\"\n",
        "    function that uses .translate() method of the model to translate german sentence into english\n",
        "    params:\n",
        "        sentence: tokenized gernam sentence\n",
        "    \"\"\"\n",
        "    sent_vec = SRC.process([sentence]).to(device)\n",
        "    input = torch.zeros((max_size, 1)).type(torch.LongTensor).to(device)\n",
        "    input += SRC.vocab.stoi['<sos>']\n",
        "    translation_idx, weights = model.translate(sent_vec, input)\n",
        "    vyvod = []\n",
        "    for t in translation_idx:\n",
        "        if t[0].max(0)[1] != EOS_IDX:\n",
        "            vyvod.append(TRG.vocab.itos[t[0].max(0)[1]])\n",
        "        else:\n",
        "            break\n",
        "    return vyvod, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cb0836b8-135a-40be-c279-93d923cee6a6",
        "id": "P_xF2e6rrqf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"def translate(sentence):\n",
        "    sent_vec = SRC.process([sentence]).to(device)\n",
        "    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\n",
        "    input += SRC.vocab.stoi['<sos>']\n",
        "    output = model(sent_vec, input, 0)\n",
        "    vyvod = []\n",
        "    for t in output:\n",
        "        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\n",
        "            vyvod.append(TRG.vocab.itos[t[0].max(0)[1]])\n",
        "        else:\n",
        "            vyvod.append(SRC.vocab.stoi['<eos>'])\n",
        "            break\n",
        "    return vyvod\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def translate(sentence):\\n    sent_vec = SRC.process([sentence]).to(device)\\n    input = torch.zeros((10, 1)).type(torch.LongTensor).to(device)\\n    input += SRC.vocab.stoi['<sos>']\\n    output = model(sent_vec, input, 0)\\n    vyvod = []\\n    for t in output:\\n        if t[0].max(0)[1] != SRC.vocab.stoi['<eos>']:\\n            vyvod.append(TRG.vocab.itos[t[0].max(0)[1]])\\n        else:\\n            vyvod.append(SRC.vocab.stoi['<eos>'])\\n            break\\n    return vyvod\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sP2TiDm18gyi",
        "outputId": "b0b33fc3-c753-4a91-e09d-189a2e237485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "translate([\"ein\", \"klein\", \"apfel\"])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<unk>', 'a', 'a', '.'], [tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0'), tensor([[[0.1548]],\n",
              "  \n",
              "          [[0.1895]],\n",
              "  \n",
              "          [[0.2284]],\n",
              "  \n",
              "          [[0.2412]],\n",
              "  \n",
              "          [[0.1862]]], device='cuda:0')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tBrHtQ_qrSKX"
      },
      "source": [
        "ИИИИ давайте также научимся считать самую популярную метрику для перевода -- BLEU (https://en.wikipedia.org/wiki/BLEU)\n",
        "\n",
        "В общем-то, вам повезло -- ее писать руками скучно, да и nltk ее написало за вас:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFIVyXXeJrCr",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def compute_bleu(inp_lines, out_lines):\n",
        "    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n",
        "    translations = [[translate(line)[0]] for line in inp_lines]\n",
        "    return corpus_bleu(translations, out_lines) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXMoE1b9O3dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [[SRC.vocab.itos[j] for j in next(iter(test_iterator)).src[:, i]] for i in range(64)]\n",
        "b = [[TRG.vocab.itos[j] for j in next(iter(test_iterator)).trg[:, i]] for i in range(64)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4bi1oJ_brjkO",
        "outputId": "ae7ecfeb-86ea-4cb3-96b7-bbf0656b85b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "compute_bleu(a, b)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.181764369609057"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2TXwm-WajUT",
        "colab_type": "code",
        "outputId": "0b07b7be-eed9-4f02-923f-77078c34c01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "corpus_bleu([[['man', 'works']], [['man', 'works']]], [['man', 'works', 'cdssdcsd', 'dcscsdc', 'shit'], ['man', 'works']]) * 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.1441569283882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuf44w8R9WnV",
        "colab_type": "text"
      },
      "source": [
        "Если вы реализовали несколько методов аттеншена, опишите каждый из них и полученную метрику на нем в отчете.\n",
        "\n",
        "<Место для вашего отчета>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v7kSMdyE_sE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = next(iter(test_iterator)).src[:, 10]\n",
        "target = next(iter(test_iterator)).trg[:, 10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9nDGJB_GaKD",
        "colab_type": "code",
        "outputId": "f79e02de-b008-4e95-99dc-8e7a323a4ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "[TRG.vocab.itos[j] for j in target]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'a',\n",
              " 'brown',\n",
              " 'and',\n",
              " 'white',\n",
              " 'dog',\n",
              " 'fetching',\n",
              " 'a',\n",
              " 'toy',\n",
              " '.',\n",
              " '<eos>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGw6mYlnGk-M",
        "colab_type": "code",
        "outputId": "a9bc42c9-9c7e-48cd-93d4-35e960d0d0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "translate([SRC.vocab.itos[j] for j in source])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<unk>', 'a', 'brown', 'and', 'white', 'dog', 'is', 'a', 'a', 'a'],\n",
              " [tensor([[[4.7126e-06]],\n",
              "  \n",
              "          [[1.0459e-04]],\n",
              "  \n",
              "          [[2.1630e-03]],\n",
              "  \n",
              "          [[1.6098e-02]],\n",
              "  \n",
              "          [[9.8151e-01]],\n",
              "  \n",
              "          [[1.1204e-04]],\n",
              "  \n",
              "          [[2.4353e-08]],\n",
              "  \n",
              "          [[5.1155e-11]],\n",
              "  \n",
              "          [[5.3041e-09]],\n",
              "  \n",
              "          [[7.8861e-07]],\n",
              "  \n",
              "          [[4.1184e-06]]], device='cuda:0'), tensor([[[7.3683e-09]],\n",
              "  \n",
              "          [[5.5191e-08]],\n",
              "  \n",
              "          [[7.3637e-07]],\n",
              "  \n",
              "          [[1.0096e-05]],\n",
              "  \n",
              "          [[3.1674e-05]],\n",
              "  \n",
              "          [[9.8108e-06]],\n",
              "  \n",
              "          [[1.6897e-04]],\n",
              "  \n",
              "          [[9.9860e-01]],\n",
              "  \n",
              "          [[1.1803e-03]],\n",
              "  \n",
              "          [[8.9244e-07]],\n",
              "  \n",
              "          [[8.0167e-11]]], device='cuda:0'), tensor([[[5.7840e-07]],\n",
              "  \n",
              "          [[2.1431e-06]],\n",
              "  \n",
              "          [[1.1678e-05]],\n",
              "  \n",
              "          [[3.1144e-05]],\n",
              "  \n",
              "          [[8.6205e-06]],\n",
              "  \n",
              "          [[2.4195e-06]],\n",
              "  \n",
              "          [[6.0253e-06]],\n",
              "  \n",
              "          [[9.6587e-01]],\n",
              "  \n",
              "          [[3.4057e-02]],\n",
              "  \n",
              "          [[8.9784e-06]],\n",
              "  \n",
              "          [[1.3782e-09]]], device='cuda:0'), tensor([[[5.8579e-09]],\n",
              "  \n",
              "          [[1.3598e-08]],\n",
              "  \n",
              "          [[4.5628e-08]],\n",
              "  \n",
              "          [[1.4839e-07]],\n",
              "  \n",
              "          [[2.1603e-07]],\n",
              "  \n",
              "          [[9.6137e-09]],\n",
              "  \n",
              "          [[7.4936e-09]],\n",
              "  \n",
              "          [[1.0586e-03]],\n",
              "  \n",
              "          [[9.9753e-01]],\n",
              "  \n",
              "          [[1.4135e-03]],\n",
              "  \n",
              "          [[4.5097e-08]]], device='cuda:0'), tensor([[[8.7604e-09]],\n",
              "  \n",
              "          [[9.8891e-09]],\n",
              "  \n",
              "          [[1.3829e-08]],\n",
              "  \n",
              "          [[3.0377e-08]],\n",
              "  \n",
              "          [[8.9475e-08]],\n",
              "  \n",
              "          [[1.0482e-09]],\n",
              "  \n",
              "          [[1.0039e-08]],\n",
              "  \n",
              "          [[4.9353e-04]],\n",
              "  \n",
              "          [[7.0313e-01]],\n",
              "  \n",
              "          [[2.9629e-01]],\n",
              "  \n",
              "          [[8.5946e-05]]], device='cuda:0'), tensor([[[9.6525e-07]],\n",
              "  \n",
              "          [[5.4880e-07]],\n",
              "  \n",
              "          [[2.4697e-07]],\n",
              "  \n",
              "          [[1.0011e-07]],\n",
              "  \n",
              "          [[1.0005e-07]],\n",
              "  \n",
              "          [[2.7501e-10]],\n",
              "  \n",
              "          [[2.6622e-10]],\n",
              "  \n",
              "          [[6.6572e-06]],\n",
              "  \n",
              "          [[2.0253e-01]],\n",
              "  \n",
              "          [[7.9525e-01]],\n",
              "  \n",
              "          [[2.2112e-03]]], device='cuda:0'), tensor([[[5.4268e-03]],\n",
              "  \n",
              "          [[1.4027e-03]],\n",
              "  \n",
              "          [[1.8113e-04]],\n",
              "  \n",
              "          [[2.3246e-05]],\n",
              "  \n",
              "          [[6.0097e-06]],\n",
              "  \n",
              "          [[9.7474e-09]],\n",
              "  \n",
              "          [[7.0538e-09]],\n",
              "  \n",
              "          [[3.7412e-06]],\n",
              "  \n",
              "          [[8.4162e-02]],\n",
              "  \n",
              "          [[8.3401e-01]],\n",
              "  \n",
              "          [[7.4783e-02]]], device='cuda:0'), tensor([[[9.5309e-02]],\n",
              "  \n",
              "          [[1.5600e-02]],\n",
              "  \n",
              "          [[1.1426e-03]],\n",
              "  \n",
              "          [[9.4223e-05]],\n",
              "  \n",
              "          [[1.9371e-05]],\n",
              "  \n",
              "          [[9.5526e-08]],\n",
              "  \n",
              "          [[2.4679e-07]],\n",
              "  \n",
              "          [[1.9427e-05]],\n",
              "  \n",
              "          [[1.6115e-02]],\n",
              "  \n",
              "          [[6.8203e-01]],\n",
              "  \n",
              "          [[1.8967e-01]]], device='cuda:0'), tensor([[[8.9495e-01]],\n",
              "  \n",
              "          [[9.8336e-02]],\n",
              "  \n",
              "          [[3.6213e-03]],\n",
              "  \n",
              "          [[4.2733e-05]],\n",
              "  \n",
              "          [[2.9071e-07]],\n",
              "  \n",
              "          [[8.6915e-09]],\n",
              "  \n",
              "          [[2.2601e-08]],\n",
              "  \n",
              "          [[3.6551e-05]],\n",
              "  \n",
              "          [[4.4190e-04]],\n",
              "  \n",
              "          [[1.4994e-03]],\n",
              "  \n",
              "          [[1.0737e-03]]], device='cuda:0')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbaEJh4gG_gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}